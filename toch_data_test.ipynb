{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yi\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from torch_data import UserCommentDataset\n",
    "from train_config import targets, data_path_config\n",
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "from utils import build_tok2idx, build_embedding_matrix\n",
    "hierarchical_model_config = {\n",
    "    'max_sentences': 240,\n",
    "    'max_sentence_length': 40\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_special_tokens(sent):\n",
    "    tokens = {'*', '^', 'O'}\n",
    "    for token in tokens:\n",
    "        sent = sent.replace(token, '')\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_error_seg(sent):\n",
    "    repls = [('(是\\s吗\\s*){1,}', '是吗'),\n",
    "             ('(去\\s吧\\s*){1,}', '去吧'),\n",
    "             ('(坑\\s爹\\s*){1,}', '坑爹')]\n",
    "    for pattern, repl in repls:\n",
    "        sent = re.sub(pattern, repl, sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_sentence(sent):\n",
    "    splits = ';+|。+|！+|…+|\\r+|\\n+|，+|~+|!+|\\s{2,}|,+|\\.{1,}|～+|、+|\\'+|≡+|·+|-+|〜'\n",
    "    return re.split(splits, sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_sentence(sents):\n",
    "    processed_sents = []\n",
    "    void_set = {' ', '', '*', '^', 'O'}\n",
    "    for sent in sents:\n",
    "        sent = sent.lstrip(' ').rstrip(' ')\n",
    "        if sent in void_set:\n",
    "            continue\n",
    "        processed_sents.append(sent)\n",
    "    return processed_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_dups(sents):\n",
    "    processed_sents = []\n",
    "    appeared_sents = set()\n",
    "    for sent in sents:\n",
    "        if sent in appeared_sents:\n",
    "            continue\n",
    "        appeared_sents.add(sent)\n",
    "        processed_sents.append(sent)\n",
    "    return processed_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformers = [remove_special_tokens, process_error_seg, split_sentence, process_sentence, remove_dups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = UserCommentDataset(data_path_config['train_data_path'], targets, content='jieba_seg', transformers= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents, tag_cat = zip(*sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 0, 0, 0], dtype=int8),\n",
       " array([1, 0, 0, 0], dtype=int8),\n",
       " array([1, 0, 0, 0], dtype=int8),\n",
       " array([1, 0, 0, 0], dtype=int8),\n",
       " array([0, 0, 0, 1], dtype=int8),\n",
       " array([1, 0, 0, 0], dtype=int8),\n",
       " array([1, 0, 0, 0], dtype=int8),\n",
       " array([1, 0, 0, 0], dtype=int8),\n",
       " array([1, 0, 0, 0], dtype=int8),\n",
       " array([0, 0, 0, 1], dtype=int8),\n",
       " array([1, 0, 0, 0], dtype=int8),\n",
       " array([1, 0, 0, 0], dtype=int8),\n",
       " array([1, 0, 0, 0], dtype=int8),\n",
       " array([1, 0, 0, 0], dtype=int8),\n",
       " array([1, 0, 0, 0], dtype=int8),\n",
       " array([1, 0, 0, 0], dtype=int8),\n",
       " array([0, 0, 0, 1], dtype=int8),\n",
       " array([1, 0, 0, 0], dtype=int8),\n",
       " array([0, 0, 0, 1], dtype=int8),\n",
       " array([1, 0, 0, 0], dtype=int8)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_cat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "max_len_sent = ''\n",
    "max_idx = 0\n",
    "for idx, sent in enumerate(sents):\n",
    "    for words in sent:\n",
    "        n_words = len([x for x in words.split(' ') if x != ''])\n",
    "        if n_words > max_len:\n",
    "            print(words)\n",
    "            max_len = n_words\n",
    "            max_len_sent = words\n",
    "            max_idx = idx\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(data_path_config['train_data_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['content'].values[max_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['jieba_seg'].values[max_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re.split(splits, sents[86749])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents[29505]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform(sents):\n",
    "    processed_sents = []\n",
    "    for sent in sents:\n",
    "        sent = sent.lstrip(' ').rstrip(' ')\n",
    "        if sent == ' ' or sent == '':\n",
    "            continue\n",
    "        print(sent)\n",
    "        processed_sents.append(sent)\n",
    "    return processed_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform(re.split(splits, sents[86749]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re.split(splits, sents[86749])[50].lstrip(' ').rstrip(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent = \" 吼吼 吼 ， 萌死 人 的 棒棒糖 ， 中 了 大众 点评 的 霸王餐 ， 太 可爱 了 。 一直 就 好奇 这个 棒棒糖 是 怎么 个 东西 ， 大众 点评 给 了 我 这个 土老冒 一个 见识 的 机会 。 看 介绍 棒棒糖 是 用 德国 糖 做 的 ， 不会 很甜 ， 中间 的 照片 是 糯米 的 ， 能 食用 ， 真是太 高端 大气 上档次 了 ， 还 可以 买 蝴蝶结 扎口 ， 送人 可以 买 礼盒 。 我 是 先 打 的 卖家 电话 ， 加 了 微信 ， 给 卖家 传 的 照片 。 等 了 几天 ， 卖家 就 告诉 我 可以 取货 了 ， 去 大官 屯 那取 的 。 虽然 连 卖家 的 面 都 没 见到 ， 但是 还是 谢谢 卖家 送 我 这么 可爱 的 东西 ， 太 喜欢 了 ， 这 哪 舍得吃 啊 。 \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove_token(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent = sent.lstrip('\"').rstrip('\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jieba.lcut('\"位于愉景花园斜对面，交通方便，环境一般般，味道一般般，*^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^**^O^*\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'asd  asd'.replace('a', 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent = '是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗 是 吗'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re.sub('(是\\s吗\\s*){1,}','是吧',sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.zeros((len(sample), 240, 100), dtype='int16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 ** 16 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence2array(data, tok2idx):\n",
    "    max_sentences = hierarchical_model_config['max_sentences']\n",
    "    max_words = hierarchical_model_config['max_sentence_length']\n",
    "    n_sentences = len(data)\n",
    "    sentence_array = np.zeros((n_sentences, max_sentences, max_words), dtype='int32')\n",
    "    for row_idx, sentences in enumerate(data):\n",
    "        for sent_idx, sentence in enumerate(sentences):\n",
    "            if sent_idx >= max_sentences:\n",
    "                continue\n",
    "            words_list = sentence.split(' ')\n",
    "            print(words_list)\n",
    "            for word_idx, word in enumerate(words_list):\n",
    "                if word_idx >= max_words:\n",
    "                    print(word_idx)\n",
    "                    continue\n",
    "                if word not in tok2idx:\n",
    "                    continue\n",
    "                print(tok2idx[word])\n",
    "                sentence_array[row_idx, sent_idx, word_idx] = tok2idx[word]\n",
    "    return sentence_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load_word2vec_format(data_path_config['embedding_path'],\n",
    "                                            binary=True, unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok2idx = build_tok2idx(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = sentence2array([sents[0]], tok2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss[sss<0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok2idx['萌死']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'棒棒糖' in tok2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'萌死 人 的 棒棒糖'.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
