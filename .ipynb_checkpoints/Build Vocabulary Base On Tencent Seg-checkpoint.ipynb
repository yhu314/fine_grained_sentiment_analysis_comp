{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build up a dictionary for all the words in the seged files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_paths = ['../datasets/train_data_jieba.txt',\n",
    "              '../datasets/valid_data_jieba.txt',\n",
    "              '../datasets/test_data_jieba.txt']\n",
    "\n",
    "tencemt_embedding_file = '../tencent_embedding/Tencent_AILab_ChineseEmbedding.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for path in file_paths:\n",
    "    file = open(path, 'r', encoding='utf8')\n",
    "    for line in file.readlines():\n",
    "        for word in line.rstrip('\\n').split(' '):\n",
    "            vocab.add(word)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = len(vocab)\n",
    "embedding_dim = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok2idx = {k:v+1 for (v, k) in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build up embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.randn(n_words+1, embedding_dim)\n",
    "embedding_matrix[0, :] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 word processed\n",
      "20000 word processed\n",
      "30000 word processed\n",
      "40000 word processed\n",
      "50000 word processed\n",
      "60000 word processed\n",
      "70000 word processed\n",
      "80000 word processed\n",
      "90000 word processed\n",
      "100000 word processed\n",
      "110000 word processed\n",
      "120000 word processed\n",
      "130000 word processed\n",
      "140000 word processed\n",
      "150000 word processed\n",
      "160000 word processed\n",
      "170000 word processed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-455e2a8d64e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0membedding_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtok2idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mword_counter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mfound_vocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "word_counter = 0\n",
    "found_vocab = set()\n",
    "with open(tencemt_embedding_file, 'rb') as handle:\n",
    "    while True:\n",
    "        line = handle.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            line = line.decode('utf8')\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        line_split = line.rstrip('\\n').split(' ')\n",
    "        \n",
    "        word, embed_list = line_split[0], line_split[1:]\n",
    "        \n",
    "        if word not in vocab:\n",
    "            continue\n",
    "            \n",
    "        if len(embed_list) != embedding_dim:\n",
    "            continue\n",
    "            \n",
    "        embedding_matrix[tok2idx[word], :] = np.array(embed_list)\n",
    "        word_counter += 1\n",
    "        found_vocab.add(word)\n",
    "        if word_counter % 10000 == 0:\n",
    "            print('{} word processed'.format(word_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = '../datasets/vocab.pickle'\n",
    "embedding_matrix_path = '../datasets/tencent_embedding.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(vocab_path, 'wb') as handle:\n",
    "    pickle.dump(tok2idx, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(embedding_matrix_path, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab - found_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
