{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = ['../datasets/train_data_jieba.txt', '../datasets/valid_data_jieba.txt', '../datasets/test_data_jieba.txt']\n",
    "X_train = []\n",
    "X_valid = []\n",
    "X_test = []\n",
    "\n",
    "with open('../datasets/train_data_jieba.txt', 'r', encoding='utf8') as handle:\n",
    "    for line in handle.readlines():\n",
    "        X_train.append(line.rstrip('\\n'))\n",
    "        \n",
    "        \n",
    "with open('../datasets/valid_data_jieba.txt', 'r', encoding='utf8') as handle:\n",
    "    for line in handle.readlines():\n",
    "        X_valid.append(line.rstrip('\\n'))\n",
    "        \n",
    "        \n",
    "with open('../datasets/test_data_jieba.txt', 'r', encoding='utf8') as handle:\n",
    "    for line in handle.readlines():\n",
    "        X_test.append(line.rstrip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ['location_traffic_convenience',\n",
    "       'location_distance_from_business_district', 'location_easy_to_find',\n",
    "       'service_wait_time', 'service_waiters_attitude',\n",
    "       'service_parking_convenience', 'service_serving_speed', 'price_level',\n",
    "       'price_cost_effective', 'price_discount', 'environment_decoration',\n",
    "       'environment_noise', 'environment_space', 'environment_cleaness',\n",
    "       'dish_portion', 'dish_taste', 'dish_look', 'dish_recommendation',\n",
    "       'others_overall_experience', 'others_willing_to_consume_again']\n",
    "train_data = pd.read_csv('../datasets/sentiment_analysis_trainingset.csv')\n",
    "valid_data = pd.read_csv('../datasets/sentiment_analysis_validationset.csv')\n",
    "\n",
    "y_train = []\n",
    "y_valid = []\n",
    "for N in name:\n",
    "    Y_train = train_data[[N]]\n",
    "    Y_train = np.array(Y_train)+2\n",
    "    Y_train = to_categorical(Y_train, num_classes=4)\n",
    "    y_train.append(Y_train)\n",
    "    \n",
    "    Y_valid = valid_data[[N]]\n",
    "    Y_valid = np.array(Y_valid)+2\n",
    "    Y_valid = to_categorical(Y_valid, num_classes=4)\n",
    "    y_valid.append(Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_path = '../datasets/tencent_embedding.npy'\n",
    "# 词典大小，embedding矩阵大小\n",
    "tok2idx_path = '../datasets/vocab.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.load(embedding_path)\n",
    "with open(tok2idx_path, 'rb') as file:\n",
    "    tok2idx = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence, tok2idx):\n",
    "    return [tok2idx[word] for word in sentence.split(' ')]\n",
    "\n",
    "maxlen = 1024\n",
    "train = [tokenize(x, tok2idx) for x in X_train]\n",
    "valid = [tokenize(x, tok2idx) for x in X_valid]\n",
    "test = [tokenize(x, tok2idx) for x in X_test]\n",
    "x_train = pad_sequences(train, maxlen=maxlen, padding='pre')\n",
    "x_valid = pad_sequences(valid, maxlen=maxlen, padding='pre')\n",
    "x_test = pad_sequences(test, maxlen=maxlen, padding='pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Builde Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Input, GlobalAveragePooling1D, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(input_dim=embedding_matrix.shape[0],\n",
    "                            output_dim=embedding_matrix.shape[1],\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_input = Input((maxlen,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_embed = embedding_layer(sentence_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv1 = Conv1D(10, 3, activation='relu')(sentence_embed)\n",
    "conv2 = Conv1D(10, 3, activation='relu')(conv1)\n",
    "conv3 = Conv1D(10, 3, activation='relu')(conv2)\n",
    "flat = GlobalAveragePooling1D()(conv3)\n",
    "\n",
    "out_list = []\n",
    "for target in name:\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
