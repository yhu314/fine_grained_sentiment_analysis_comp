{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build up a dictionary for all the words in the seged files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_paths = ['../datasets/train_data_jieba.txt',\n",
    "              '../datasets/valid_data_jieba.txt',\n",
    "              '../datasets/test_data_jieba.txt']\n",
    "\n",
    "tencemt_embedding_file = '../tencent_embedding/Tencent_AILab_ChineseEmbedding.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for path in file_paths:\n",
    "    file = open(path, 'r', encoding='utf8')\n",
    "    for line in file.readlines():\n",
    "        for word in line.rstrip('\\n').split(' '):\n",
    "            vocab.add(word)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = len(vocab)\n",
    "embedding_dim = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok2idx = {k:v+1 for (v, k) in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build up embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.randn(n_words+1, embedding_dim)\n",
    "embedding_matrix[0, :] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 word processed\n",
      "20000 word processed\n",
      "30000 word processed\n",
      "40000 word processed\n",
      "50000 word processed\n",
      "60000 word processed\n",
      "70000 word processed\n",
      "80000 word processed\n",
      "90000 word processed\n",
      "100000 word processed\n",
      "110000 word processed\n",
      "120000 word processed\n",
      "130000 word processed\n",
      "140000 word processed\n",
      "150000 word processed\n",
      "160000 word processed\n",
      "170000 word processed\n",
      "180000 word processed\n",
      "190000 word processed\n",
      "200000 word processed\n",
      "210000 word processed\n",
      "220000 word processed\n",
      "230000 word processed\n",
      "240000 word processed\n",
      "250000 word processed\n",
      "260000 word processed\n",
      "270000 word processed\n",
      "280000 word processed\n",
      "290000 word processed\n",
      "300000 word processed\n",
      "310000 word processed\n",
      "320000 word processed\n",
      "330000 word processed\n",
      "340000 word processed\n",
      "350000 word processed\n",
      "360000 word processed\n",
      "370000 word processed\n",
      "380000 word processed\n",
      "390000 word processed\n",
      "400000 word processed\n"
     ]
    }
   ],
   "source": [
    "word_counter = 0\n",
    "found_vocab = set()\n",
    "with open(tencemt_embedding_file, 'rb') as handle:\n",
    "    while True:\n",
    "        line = handle.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            line = line.decode('utf8')\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        line_split = line.rstrip('\\n').split(' ')\n",
    "        \n",
    "        word, embed_list = line_split[0], line_split[1:]\n",
    "        \n",
    "        if word not in vocab:\n",
    "            continue\n",
    "            \n",
    "        if len(embed_list) != embedding_dim:\n",
    "            continue\n",
    "            \n",
    "        embedding_matrix[tok2idx[word], :] = np.array(embed_list)\n",
    "        word_counter += 1\n",
    "        found_vocab.add(word)\n",
    "        if word_counter % 10000 == 0:\n",
    "            print('{} word processed'.format(word_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = '../datasets/vocab.pickle'\n",
    "embedding_matrix_path = '../datasets/tencent_embedding.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(vocab_path, 'wb') as handle:\n",
    "    pickle.dump(tok2idx, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(embedding_matrix_path, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16705"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab - found_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
